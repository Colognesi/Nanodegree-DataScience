Creating Metrics
Funnels
There are additional concepts and terms that are commonly used for designing experiments, especially for web-based studies. In a web experiment, you'll often think of the user funnel. A funnel is the flow of steps you expect a user of your product to take. Typically, the funnel ends at the place where your main evaluation metric is recorded, and includes a step where your experimental manipulation can be performed. For example, we might think of the following steps for someone to purchase a product in an online store:

Visit the site homepage
Search for a desired product or click on a product category
Click on a product image
Add the product to the cart
Check out and finalize purchase
One property to note about user funnels is that typically there will be some dropoff in the users that move from step to step. 
This is much like how an actual funnel narrows from a large opening to a small exit. Outside of an experiment, funnels can be used to 
analyze user flows. Observations from these flows can then be used to motivate experiments to try and improve the dropoff rates.

It's also worth noting that the flow through a funnel might be idealized compared to actual user practice. In the above example, 
users might perform multiple searches in a single session, or want to purchase multiple things. A user might access the site through 
a specific link, subverting the top part of the funnel. Refining the funnel and being specific about the kinds of events that are 
expected can help you create a consistent, reliable design and analysis.

Unit of Diversion
Once you have a funnel, think about how you can implement your experimental manipulation in the funnel. If the goal of the above 
experiment was to change the way the site looks after a user clicks on a product image, we need to figure out a way to assign users 
to either a control group or experimental group. The place in which you make this assignment is known as the unit of diversion. 
Depending on the type of experiment you have, you might have different options for diversion, each with its own pros and cons:

Event-based diversion (e.g. pageview): Each time a user loads up the page of interest, the experimental condition is randomly rolled. 
Since this ignores previous visits, this can create an inconsistent experience, if the condition causes a user-visible change.
Cookie-based diversion: A cookie is stored on the user's device, which determines their experimental condition as long as the 
cookie remains on the device. Cookies don't require a user to have an account or be logged in, but can be subverted through 
anonymous browsing or a user just clearing out cookies.

Account-based diversion (e.g. User ID): User IDs are randomly divided into conditions. Account-based diversions are reliable, 
but requires users to have accounts and be logged in. This means that our pool of data might be limited in scope, and you'll 
need to consider the risks of using personally-identifiable information.
When it comes to selecting a unit of diversion, the consistency of the experience required can be a major factor to consider. 
For the example provided, we need something more consistent than pageview events. So we then consider the cookie-based diversion. 
If the differences in interface between control and experiment are fairly minor, then we're probably okay with cookie-based diversion. 
But if we think that users will notice the change and we believe that it will have a major effect on experience, then we might be 
inclined to choose an account-based diversion.

Invariant and Evaluation Metrics
A funnel will also be of benefit when it comes to deciding on metrics to track and analyze as part of the experiment. 
The immediate features that come out of a funnel come in the form of counts and ratios. For example, we could count the 
number of times a search results in a product being selected (a count), or the ratio of selections to searches as adjacent 
slices in the funnel (a ratio).

There are two major categories that we can consider features: as evaluation metrics or as invariant metrics. Evaluation 
metrics were mentioned in the previous page as the metrics by which we compare groups. Ideally, we hope to see a difference 
between groups that will tell us if our manipulation was a success. We might want to see an increased click-through-rate from 
search results to products, or an increase in overall revenue. On the flip side, invariant metrics are metrics that we hope will 
not be different between groups. Metrics in this category serve to check that the experiment is running as expected. For example,
 in an experiment with cookie-based diversion, the number of cookies generated for each condition would be a good invariant metric. 
Another metric could compare the distribution of times in which cookies were generated, to check the bias in the randomization procedure.

We're not limited to tracking just one metric of each type. It's not unusual to track multiple invariant metrics as checks on the 
system's integrity, or multiple evaluation metrics to check different potential facets of a manipulation's effects. Don't think 
that you need to track every possible metric, however. It's better to focus on a few key metrics, ignoring features that might be 
less reliable or highly correlated to other, more informative features. We'll discuss statistical considerations surrounding metrics 
in the next lesson.

============================================================================================================================================


Controlling Variables
If we want to determine causality between two features, there are two main things to control. First of all, we need to enact the 
manipulation on one of the features of interest, so that we know that it is causing the change in the other feature. In order to 
know that it was our manipulated variable and not any other, the second major control point is that we want to make sure that all 
other features are accounted for. These two requirements make the arguments for causality much stronger with an experiment compared 
to a quasi-experiment or observational study.

If we aren't able to control all features or there is a lack of equivalence between groups, then we may be susceptible to confounding 
variables. The correlation observed between two variables might be due to changes in a third variable, rather than one causing the other. 
Another possibility is that there is a causal relationship between the two features, but it is an indirect relationship mediated by a third, 
intermediate variable. This intermediate variable might be a larger driver of the changes in the output, with the manipulated variable 
only having a direct effect on the intermediate feature.

For the case where we see a relationship but don't perform a manipulation, we also need to be careful about the direction of effect. 
A relationship between variables "A" and "B" might be due to "A" having an effect on "B" or the reverse, "B" having an effect on "A". 
It might even be the case that "A" and "B" are related through some other function like a third variable.

================================================================================


Checking Validity
When designing an experiment, it's important to keep in mind validity, which concerns how well conclusions can be supported. 
There are three major conceptual dimensions upon which validity can be assessed:

Construct Validity
Internal Validity
External Validity

Construct Validity
Construct validity is tied to the earlier discussion of how well one's goals are aligned to the evaluation metrics used to evaluate it.
 Poor construct validity can come about when an evaluation metric does not actually measure something related to the desired outcome concept.
 Alternatively, it might be that a metric is ill-constructed, such that it does not make clear distinctions on the outcome concept.

Internal Validity
Internal validity refers to the degree to which a causal relationship can be derived from an experiment's results. 
Controlling for and accounting for other variables is key to maintaining good internal validity. The previous page on controlling
 variables shows ways in which internal validity might not be met.

External Validity
External validity is concerned with the ability of an experimental outcome to be generalized to a broader population. 
This is most relevant with experiments that involve sampling: how representative is the sample to the whole? For studies at 
academic institutions, a frequent question is if data collected using only college students can be generalized to other age 
or socioeconomic groups.

====================================================

Checking Bias
Biases in experiments are systematic effects that interfere with the interpretation of experimental results, mostly in terms 
of internal validity. Just as humans can have a lot of different biases, there are numerous ways in which an experiment can become unbalanced.

Sampling Bias
Many experimental biases fall under the sampling bias umbrella. Sampling biases are those that cause our observations to not 
be representative of the population. For example, if assignment to experimental groups is done in an arbitrary fashion 
(as opposed to random assignment or matched groups), we risk our outcomes being based less on the experimental manipulation 
and more on the composition of the underlying groups.

Studies that use surveys to collect data often have to deal with the self-selection bias. The types of people that respond to a 
survey might be qualitatively very different from those that do not. A straight average of responses would not necessarily reflect 
the feelings of the full population; weighting responses based on the differences between the observed responses and properties of 
the target population may be needed to come to reasonable conclusions.

One type of sampling bias related to missing data is the survivor bias. Survivor bias is one where losses or dropout of observed 
units is not accounted for in an analysis. A key example of this was in British World War II operations research, where engineers 
avoided using survivor bias when they considered where to add armor to their planes. Rather than add armor to the spots where 
returning planes had bullet holes, armor was added to the spots where the planes didn't have bullet holes. That's because the planes 
that took shots to those places probably crashed, due to those locations being more vital for maintaining flight, so they didn't 
"survive" and weren't available for observation.

Novelty Bias
A novelty effect is one that causes observers to change their behavior simply because they're seeing something new. We might not be 
able to gauge the true effect of a manipulation until after the novelty wears off and population metrics return to a level that 
actually reflects the changes made. This will be important for cases where we want to track changes over time, such as trying to 
get users to re-visit a webpage or use an app more frequently. Novelty is probably not a concern (or perhaps what we hope for) 
when it comes to manipulations that are expected to only have a one-shot effect.

Order Biases
There are a couple of biases to be aware of when running a within-subjects experiment. Recall that in a within-subjects design, 
each participant performs a task or makes a rating in multiple experimental conditions, rather than just one. The order in which 
conditions are completed could have an effect on participant responses. A primacy effect is one that affects early conditions, 
perhaps biasing them to be recalled better or to serve as anchor values for later conditions. A recency effect is one that affects 
later conditions, perhaps causing bias due to being fresher in memory or task fatigue.

An easy way of getting around order biases is to simply randomize the order of conditions. If we have three conditions, then 
each of the six ways of completing the task (ABC, ACB, BAC, BCA, CAB, CBA) should be equally likely. While there still might 
end up being order effects like carry-over effects, where a particular condition continues to have an effect on future conditions, 
this will be much easier to detect than if every participant completed the task in the exact same order of conditions.

Experimenter Bias
One bias to watch out for, especially in face-to-face experiments, is the experimenter bias. This is where the presence or 
knowledge of the experimenter can affect participants' behaviors or performance. If an experimenter knows what condition a 
participant is in, they might subtly nudge the participant towards their expected result with their interactions with the participant. 
In addition, participants may act differently in the presence of an experimenter, to try and act in the 'right' way – regardless of 
if a subject actually knows what the experimenter is looking for or not.

This is where design steps like blinding are important. In blinding, the administrator of a procedure or the participant do not know 
the condition being used, to avoid that subconscious bias from having an effect. In particular, the double-blind design hides condition
 information from both the administrator and participant in order to have a strong rein on experimenter-based biases.