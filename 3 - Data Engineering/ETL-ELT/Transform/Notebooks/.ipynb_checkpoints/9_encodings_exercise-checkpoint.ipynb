{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings\n",
    "\n",
    "Encodings are a set of rules mapping string characters to their binary representations. Python supports dozens of different encoding as seen here in [this link](https://docs.python.org/3/library/codecs.html#standard-encodings). Because the web was originally in English, the first encoding rules mapped binary code to the English alphabet. \n",
    "\n",
    "The English alphabet has only 26 letters. But other languages have many more characters including accents, tildes and umlauts. As time went on, more encodings were invented to deal with languages other than English. The utf-8 standard tries to provide a single encoding schema that can encompass all text.\n",
    "\n",
    "The problem is that it's difficult to know what encoding rules were used to make a file unless somebody tells you. The most common encoding by far is utf-8. Pandas will assume that files are utf-8 when you read them in or write them out.\n",
    "\n",
    "Run the code cell below to read in the population data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../../extract/data/csv/population_data.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas should have been able to read in this data set without any issues. Next, run the code cell below to read in the 'mystery.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a307ef123dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/mystery.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/mystery.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have gotten an error: **UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte**. This means pandas assumed the file had a utf-8 encoding but had trouble reading in the data file. \n",
    "\n",
    "Your job in the next cell is to figure out the encoding for the mystery.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://docs.python.org/3/library/codecs.html#standard-encodings'\n",
    "encodings = pd.read_html(url)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codec</th>\n",
       "      <th>Aliases</th>\n",
       "      <th>Languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cp037</td>\n",
       "      <td>IBM037, IBM039</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cp273</td>\n",
       "      <td>273, IBM273, csIBM273</td>\n",
       "      <td>German New in version 3.4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cp500</td>\n",
       "      <td>EBCDIC-CP-BE, EBCDIC-CP-CH, IBM500</td>\n",
       "      <td>Western Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cp875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cp1026</td>\n",
       "      <td>ibm1026</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cp1140</td>\n",
       "      <td>ibm1140</td>\n",
       "      <td>Western Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>utf_16</td>\n",
       "      <td>U16, utf16</td>\n",
       "      <td>all languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>utf_16_be</td>\n",
       "      <td>UTF-16BE</td>\n",
       "      <td>all languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>utf_16_le</td>\n",
       "      <td>UTF-16LE</td>\n",
       "      <td>all languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Codec                             Aliases                   Languages\n",
       "3       cp037                      IBM037, IBM039                     English\n",
       "4       cp273               273, IBM273, csIBM273  German New in version 3.4.\n",
       "7       cp500  EBCDIC-CP-BE, EBCDIC-CP-CH, IBM500              Western Europe\n",
       "26      cp875                                 NaN                       Greek\n",
       "31     cp1026                             ibm1026                     Turkish\n",
       "33     cp1140                             ibm1140              Western Europe\n",
       "91     utf_16                          U16, utf16               all languages\n",
       "92  utf_16_be                            UTF-16BE               all languages\n",
       "93  utf_16_le                            UTF-16LE               all languages"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll remember to put this on a module\n",
    "wrong_encoding = []\n",
    "for encode in encodings['Codec']:\n",
    "    try:\n",
    "        pd.read_csv('../data/mystery.csv', encoding = encode)\n",
    "    except:\n",
    "        wrong_encoding.append(encode)\n",
    "        continue\n",
    "encodings.query(\"Codec not in @wrong_encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings.aliases import aliases\n",
    "import pandas as pd\n",
    "\n",
    "def possible_encode(path, file_type = 'csv'):\n",
    "    file_type = file_type\n",
    "    alias_values = set(aliases.values())\n",
    "    \n",
    "    correct_encode = []\n",
    "    \n",
    "    for encode in alias_values:\n",
    "        try:\n",
    "            pd.read_csv(path, encoding = encode)\n",
    "            correct_encode.append(encode)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return print('Possible Encode: {}'.format([x for x in correct_encode]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Encode: ['cp037', 'cp273', 'cp1026', 'utf_16_be', 'utf_16', 'cp1140', 'utf_16_le', 'cp500']\n"
     ]
    }
   ],
   "source": [
    "possible_encode('../data/mystery.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>...</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>54211.0</td>\n",
       "      <td>55438.0</td>\n",
       "      <td>56225.0</td>\n",
       "      <td>56695.0</td>\n",
       "      <td>57032.0</td>\n",
       "      <td>57360.0</td>\n",
       "      <td>...</td>\n",
       "      <td>101353.0</td>\n",
       "      <td>101453.0</td>\n",
       "      <td>101669.0</td>\n",
       "      <td>102053.0</td>\n",
       "      <td>102577.0</td>\n",
       "      <td>103187.0</td>\n",
       "      <td>103795.0</td>\n",
       "      <td>104341.0</td>\n",
       "      <td>104822.0</td>\n",
       "      <td>105264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>8996351.0</td>\n",
       "      <td>9166764.0</td>\n",
       "      <td>9345868.0</td>\n",
       "      <td>9533954.0</td>\n",
       "      <td>9731361.0</td>\n",
       "      <td>9938414.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27294031.0</td>\n",
       "      <td>28004331.0</td>\n",
       "      <td>28803167.0</td>\n",
       "      <td>29708599.0</td>\n",
       "      <td>30696958.0</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>32758020.0</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>34656032.0</td>\n",
       "      <td>35530081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>5643182.0</td>\n",
       "      <td>5753024.0</td>\n",
       "      <td>5866061.0</td>\n",
       "      <td>5980417.0</td>\n",
       "      <td>6093321.0</td>\n",
       "      <td>6203299.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21759420.0</td>\n",
       "      <td>22549547.0</td>\n",
       "      <td>23369131.0</td>\n",
       "      <td>24218565.0</td>\n",
       "      <td>25096150.0</td>\n",
       "      <td>25998340.0</td>\n",
       "      <td>26920466.0</td>\n",
       "      <td>27859305.0</td>\n",
       "      <td>28813463.0</td>\n",
       "      <td>29784193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>1608800.0</td>\n",
       "      <td>1659800.0</td>\n",
       "      <td>1711319.0</td>\n",
       "      <td>1762621.0</td>\n",
       "      <td>1814135.0</td>\n",
       "      <td>1864791.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2947314.0</td>\n",
       "      <td>2927519.0</td>\n",
       "      <td>2913021.0</td>\n",
       "      <td>2905195.0</td>\n",
       "      <td>2900401.0</td>\n",
       "      <td>2895092.0</td>\n",
       "      <td>2889104.0</td>\n",
       "      <td>2880703.0</td>\n",
       "      <td>2876101.0</td>\n",
       "      <td>2873457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>AND</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>13411.0</td>\n",
       "      <td>14375.0</td>\n",
       "      <td>15370.0</td>\n",
       "      <td>16412.0</td>\n",
       "      <td>17469.0</td>\n",
       "      <td>18549.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83861.0</td>\n",
       "      <td>84462.0</td>\n",
       "      <td>84449.0</td>\n",
       "      <td>83751.0</td>\n",
       "      <td>82431.0</td>\n",
       "      <td>80788.0</td>\n",
       "      <td>79223.0</td>\n",
       "      <td>78014.0</td>\n",
       "      <td>77281.0</td>\n",
       "      <td>76965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Kosovo</td>\n",
       "      <td>XKX</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>947000.0</td>\n",
       "      <td>966000.0</td>\n",
       "      <td>994000.0</td>\n",
       "      <td>1022000.0</td>\n",
       "      <td>1050000.0</td>\n",
       "      <td>1078000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1747383.0</td>\n",
       "      <td>1761474.0</td>\n",
       "      <td>1775680.0</td>\n",
       "      <td>1791000.0</td>\n",
       "      <td>1805200.0</td>\n",
       "      <td>1824100.0</td>\n",
       "      <td>1821800.0</td>\n",
       "      <td>1801800.0</td>\n",
       "      <td>1816200.0</td>\n",
       "      <td>1830700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Yemen, Rep.</td>\n",
       "      <td>YEM</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>5172135.0</td>\n",
       "      <td>5260501.0</td>\n",
       "      <td>5351799.0</td>\n",
       "      <td>5446063.0</td>\n",
       "      <td>5543339.0</td>\n",
       "      <td>5643643.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22356391.0</td>\n",
       "      <td>22974929.0</td>\n",
       "      <td>23606779.0</td>\n",
       "      <td>24252206.0</td>\n",
       "      <td>24909969.0</td>\n",
       "      <td>25576322.0</td>\n",
       "      <td>26246327.0</td>\n",
       "      <td>26916207.0</td>\n",
       "      <td>27584213.0</td>\n",
       "      <td>28250420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>ZAF</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>17456855.0</td>\n",
       "      <td>17920673.0</td>\n",
       "      <td>18401608.0</td>\n",
       "      <td>18899275.0</td>\n",
       "      <td>19412975.0</td>\n",
       "      <td>19942303.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50412129.0</td>\n",
       "      <td>50970818.0</td>\n",
       "      <td>51584663.0</td>\n",
       "      <td>52263516.0</td>\n",
       "      <td>52998213.0</td>\n",
       "      <td>53767396.0</td>\n",
       "      <td>54539571.0</td>\n",
       "      <td>55291225.0</td>\n",
       "      <td>56015473.0</td>\n",
       "      <td>56717156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>3044846.0</td>\n",
       "      <td>3140264.0</td>\n",
       "      <td>3240587.0</td>\n",
       "      <td>3345145.0</td>\n",
       "      <td>3452942.0</td>\n",
       "      <td>3563407.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13082517.0</td>\n",
       "      <td>13456417.0</td>\n",
       "      <td>13850033.0</td>\n",
       "      <td>14264756.0</td>\n",
       "      <td>14699937.0</td>\n",
       "      <td>15153210.0</td>\n",
       "      <td>15620974.0</td>\n",
       "      <td>16100587.0</td>\n",
       "      <td>16591390.0</td>\n",
       "      <td>17094130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>3747369.0</td>\n",
       "      <td>3870756.0</td>\n",
       "      <td>3999419.0</td>\n",
       "      <td>4132756.0</td>\n",
       "      <td>4269863.0</td>\n",
       "      <td>4410212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13558469.0</td>\n",
       "      <td>13810599.0</td>\n",
       "      <td>14086317.0</td>\n",
       "      <td>14386649.0</td>\n",
       "      <td>14710826.0</td>\n",
       "      <td>15054506.0</td>\n",
       "      <td>15411675.0</td>\n",
       "      <td>15777451.0</td>\n",
       "      <td>16150362.0</td>\n",
       "      <td>16529904.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country Name Country Code     Indicator Name Indicator Code        1960  \\\n",
       "0           Aruba          ABW  Population, total    SP.POP.TOTL     54211.0   \n",
       "1     Afghanistan          AFG  Population, total    SP.POP.TOTL   8996351.0   \n",
       "2          Angola          AGO  Population, total    SP.POP.TOTL   5643182.0   \n",
       "3         Albania          ALB  Population, total    SP.POP.TOTL   1608800.0   \n",
       "4         Andorra          AND  Population, total    SP.POP.TOTL     13411.0   \n",
       "..            ...          ...                ...            ...         ...   \n",
       "259        Kosovo          XKX  Population, total    SP.POP.TOTL    947000.0   \n",
       "260   Yemen, Rep.          YEM  Population, total    SP.POP.TOTL   5172135.0   \n",
       "261  South Africa          ZAF  Population, total    SP.POP.TOTL  17456855.0   \n",
       "262        Zambia          ZMB  Population, total    SP.POP.TOTL   3044846.0   \n",
       "263      Zimbabwe          ZWE  Population, total    SP.POP.TOTL   3747369.0   \n",
       "\n",
       "           1961        1962        1963        1964        1965  ...  \\\n",
       "0       55438.0     56225.0     56695.0     57032.0     57360.0  ...   \n",
       "1     9166764.0   9345868.0   9533954.0   9731361.0   9938414.0  ...   \n",
       "2     5753024.0   5866061.0   5980417.0   6093321.0   6203299.0  ...   \n",
       "3     1659800.0   1711319.0   1762621.0   1814135.0   1864791.0  ...   \n",
       "4       14375.0     15370.0     16412.0     17469.0     18549.0  ...   \n",
       "..          ...         ...         ...         ...         ...  ...   \n",
       "259    966000.0    994000.0   1022000.0   1050000.0   1078000.0  ...   \n",
       "260   5260501.0   5351799.0   5446063.0   5543339.0   5643643.0  ...   \n",
       "261  17920673.0  18401608.0  18899275.0  19412975.0  19942303.0  ...   \n",
       "262   3140264.0   3240587.0   3345145.0   3452942.0   3563407.0  ...   \n",
       "263   3870756.0   3999419.0   4132756.0   4269863.0   4410212.0  ...   \n",
       "\n",
       "           2008        2009        2010        2011        2012        2013  \\\n",
       "0      101353.0    101453.0    101669.0    102053.0    102577.0    103187.0   \n",
       "1    27294031.0  28004331.0  28803167.0  29708599.0  30696958.0  31731688.0   \n",
       "2    21759420.0  22549547.0  23369131.0  24218565.0  25096150.0  25998340.0   \n",
       "3     2947314.0   2927519.0   2913021.0   2905195.0   2900401.0   2895092.0   \n",
       "4       83861.0     84462.0     84449.0     83751.0     82431.0     80788.0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "259   1747383.0   1761474.0   1775680.0   1791000.0   1805200.0   1824100.0   \n",
       "260  22356391.0  22974929.0  23606779.0  24252206.0  24909969.0  25576322.0   \n",
       "261  50412129.0  50970818.0  51584663.0  52263516.0  52998213.0  53767396.0   \n",
       "262  13082517.0  13456417.0  13850033.0  14264756.0  14699937.0  15153210.0   \n",
       "263  13558469.0  13810599.0  14086317.0  14386649.0  14710826.0  15054506.0   \n",
       "\n",
       "           2014        2015        2016        2017  \n",
       "0      103795.0    104341.0    104822.0    105264.0  \n",
       "1    32758020.0  33736494.0  34656032.0  35530081.0  \n",
       "2    26920466.0  27859305.0  28813463.0  29784193.0  \n",
       "3     2889104.0   2880703.0   2876101.0   2873457.0  \n",
       "4       79223.0     78014.0     77281.0     76965.0  \n",
       "..          ...         ...         ...         ...  \n",
       "259   1821800.0   1801800.0   1816200.0   1830700.0  \n",
       "260  26246327.0  26916207.0  27584213.0  28250420.0  \n",
       "261  54539571.0  55291225.0  56015473.0  56717156.0  \n",
       "262  15620974.0  16100587.0  16591390.0  17094130.0  \n",
       "263  15411675.0  15777451.0  16150362.0  16529904.0  \n",
       "\n",
       "[264 rows x 62 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Figure out what the encoding is of the myster.csv file\n",
    "# HINT: pd.read_csv('mystery.csv', encoding=?) where ? is the string for an encoding like 'ascii'\n",
    "# HINT: This link has a list of encodings that Python recognizes https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "\n",
    "\n",
    "# Python has a file containing a dictionary of encoding names and associated aliases\n",
    "# This line imports the dictionary and then creates a set of all available encodings\n",
    "# You can use this set of encodings to search for the correct encoding\n",
    "# If you'd like to see what this file looks like, execute the following Python code to see where the file is located\n",
    "#    from encodings import aliases\n",
    "#    aliases.__file__\n",
    "\n",
    "from encodings.aliases import aliases\n",
    "\n",
    "alias_values = set(aliases.values())\n",
    "\n",
    "# TODO: iterate through the alias_values list trying out the different encodings to see which one or ones work\n",
    "# HINT: Use a try - except statement. Otherwise your code will produce an error when reading in the csv file\n",
    "#       with the wrong encoding.\n",
    "# HINT: In the try statement, print out the encoding name so that you know which one(s) worked.\n",
    "pd.read_csv('../data/mystery.csv', encoding = 'utf_16').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "There are dozens of encodings that Python can handle; however, Pandas assumes a utf-8 encoding. This makes sense since utf-8 is very common. However, you will sometimes come across files with other encodings. If you don't know the encoding, you have to search for it.\n",
    "\n",
    "Note, as always, there is a solution file for this exercise. Go to File->Open.\n",
    "\n",
    "There is a Python library that can be of some help when you don't know an encoding: chardet. Run the code cells below to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\kaiqu\\anaconda3\\lib\\site-packages (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# install the chardet library\n",
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'UTF-16', 'confidence': 1.0, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "# import the chardet library\n",
    "import chardet \n",
    "\n",
    "# use the detect method to find the encoding\n",
    "# 'rb' means read in the file as binary\n",
    "with open(\"../data/mystery.csv\", 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
